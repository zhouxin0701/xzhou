
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Xin Zhou</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<!-- <div class="menu-item"><a href="#Publication">Publications</a></div>
<div class="menu-item"><a href="#Teaching">Teaching</a></div>
<div class="menu-item"><a href="#Awards">Awards</a></div>
<div class="menu-item"><a href="Research.html">Research</a></div> -->
</td>
<td id="layout-content">
<table class="imgtable"><tr><td>
<img src="zx.jpg" alt="" width="150px" />&nbsp;</td>
<td align="left"><p>Xin Zhou <br />
</p>Email: <a href="mailto:xzhou42@buffalo.edu">xzhou42@buffalo.edu</a>
</td></tr></table>
<h2>Biography</h2>
<p>Xin Zhou received his master's degree in the <a href="https://sse.ustc.edu.cn/mainm.htm">School of Software Engineering</a> at <a href="https://www.ustc.edu.cn/">University of Science and Technology of China (USTC)</a> in 2023.

    In 2022, he conducted his research under the supervision of Prof. <a href="https://people.ucas.ac.cn/~cwang">Can Wang</a> at the <a href="https://english.siat.ac.cn/">Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences (CAS)</a>, where he studied in the <a href="https://english.siat.ac.cn/SI2017/IAIT2017/RC1/CIB/">Intelligent Bionics Center</a>. His research interests focus on time series processing, motion intent recognition and knowledge tracing.</p>

<p>[<a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=lMCtFHsAAAAJ">Google Scholar</a>] </p>

<!-- <p>[<a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=lMCtFHsAAAAJ">Google Scholar</a>, <a href="https://github.com/zhouxin0701/xinzhou/blob/main/cv.pdf">CV</a>] </p> -->


  
</ul>


<h2 id="Awards">Honours and Awards</h2>
<ul>
<li><p>Awarded an outstanding graduate of Anhui Province and the University of Science and Technology of China
    <br /> University of Science and Technology of China, Hefei, Anhui, 2023</p>
</li>
<li><p><a href="https://www.kaggle.com/competitions/riiid-test-answer-prediction">Kaggle: Riiid Answer Correctness Prediction (Track knowledge states of 1M+ students in the wild), Bronze Medal, Top 6%, Ranking: 198/3395</a>
    <br />University of Science and Technology of China, Hefei, Anhui, 2021</p>
</li>
<li><p><a href="https://aistudio.baidu.com/competition/detail/66/0/introduction">Baidu "Machine Reading Comprehension Task" Challenge, Ranking: 9/1290</a>
    <br /> University of Science and Technology of China, Hefei, Anhui, 2021</p>
</li>
</ul>

<h2 id="Publication">Publications</h2>



<ul>
<li><p><a href="https://arxiv.org/abs/2409.07589">Multi-scale spatiotemporal representation learning for EEG-based emotion recognition</a>
<br /> <b>X. Zhou</b> and X. Peng&#42
    <br /> <a href="https://arxiv.org/abs/2409.07589">arXiv preprint arXiv:2404.07589</a>
</p></li>
</ul>

  
<ul>
<li><p><a href="https://arxiv.org/abs/2404.07517">Efficient sEMG-based Cross-Subject Joint Angle Estimation via Hierarchical Spiking Attentional Feature Decomposition Network</a>
<br /> <b>X. Zhou</b>, C. Lin, C. Wang and X. Peng&#42
    <br /> <a href="https://arxiv.org/abs/2404.07517#">arXiv preprint arXiv:2404.07517</a>
</p></li>
</ul>

<ul>
<li><p><a href="https://doi.org/10.1038/s41598-024-58590-x">Instance-level 6D pose estimation based on multi-task parameter sharing for robotic grasping</a>
<br /> L. Zhang, <b>X. Zhou</b>, J. Liu, C. Wang&#42 and X. Wu
    <br /> <a href="https://www.nature.com/srep/">Scientific Reports, 2024  (JCR Q2, IF: 4.6)</a>
</p></li>
</ul>

<ul>
<li><p><a href="https://doi.org/10.1109/LRA.2023.3235683">Continuous Estimation of Lower Limb Joint Angles From Multi-Stream Signals Based on Knowledge Tracing</a>
<br /> <b>X. Zhou</b>, C. Wang&#42, L. Zhang, J. Liu, G. Liang, and X. Wu
    <br /> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7083369">IEEE Robotics and Automation Letters, 2023  (JCR Q2, IF: 5.2)</a>
</p></li>
</ul>

<ul>
    <li><p><a href="https://doi.org/10.1017/s026357472200159x">Time–Frequency    Feature Transform Suite for Deep Learning-Based Gesture Recognition Using sEMG Signals</a>
    <br /> <b>X. Zhou</b>, J. Ye, C. Wang&#42, J. Zhong and X. Wu
        <br /> <a href="https://www.cambridge.org/core/journals/robotica">Robotica, 2022  (JCR Q3, IF: 2.7)</a>
    </p></li>
    </ul>

<ul>
<li><p><a href="https://doi.org/10.1109/LRA.2022.3185380">A Novel Method for Detecting Misclassifications of the Locomotion Mode in Lower-Limb Exoskeleton Robot Control</a>
<br /> J. Liu, <b>X. Zhou</b>, B. He, P. Li, C. Wang&#42 and X. Wu
    <br /> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7083369">IEEE Robotics and Automation Letters, 2022  (JCR Q2, IF: 5.2)</a>
</p></li>
</ul>




<style>
    .container {
        display: flex;
        justify-content: center; /* 水平居中 */
    }
    
    .small-div {
        width: 250px; /* 设置 div 的宽度 */
        height: 200px; /* 设置 div 的高度 */
    }
</style>

<div class="container">
    <div class="small-div">
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=THUz_UOHyp-WkETyjXqT4y-g1O4AjS6oZSHvwXA1e98"></script>
    </div>

  
</div>



</td>
</tr>
</table>
</body>
</html>

